syntax = "proto3";

package unreal_rl;

////////////////////////////////////////////////////////////////////////////////
//                                SPACES 
////////////////////////////////////////////////////////////////////////////////

message RangeInfo {
    float low = 1;
    float high = 2;
}

message SpaceData {
    enum ESpaceType {
        E_SPACE_TYPE_UNKNOWN = 0;
        E_SPACE_TYPE_DISCRETE = 1;
        E_SPACE_TYPE_CONTINUOUS = 2;
    }
    ESpaceType type = 1;
    repeated int32 shape = 2;
    repeated float data = 3;
    int32 discrete_n = 4;
}

////////////////////////////////////////////////////////////////////////////////
//                                Environment
////////////////////////////////////////////////////////////////////////////////

message StepRequest {
    // an action provided by the agent
    SpaceData action = 1;
}

message MultiAgentStepRequest {
    // Info for each agent involved
    repeated StepRequest requests = 1;
}

message StepResponse {
    // this will be an element of the environment’s observation_space
    SpaceData observation = 1;

    // The amount of reward returned as a result of taking the action.
    float reward = 2;

    // whether a terminal state (as defined under the MDP of the task) is reached. 
    // In this case further step() calls could return undefined results.
    bool terminated = 3;

    // whether a truncation condition outside the scope of the MDP is satisfied. 
    // Typically a timelimit, but could also be used to indicate agent physically going out of bounds. 
    // Can be used to end the episode prematurely before a terminal state is reached.
    bool truncated = 4;
}

message MultiAgentStepResponse {
    // One for each agent
    repeated StepResponse responses = 1;
}

message ResetRequest {
    // The seed that is used to initialize the environment’s PRNG. 
    // If default (0) will use some source of entropy instead.
    int32 seed = 1;

    // options...
}

message ResetResponse {
    // Observation of the initial state. This will be an element of the environment’s observation_space
    SpaceData observation = 1;
}

message GetNumAreasRequest {
}

message GetNumAreasResponse {
    int32 num_areas = 1;
}


message CloseRequest {

}

message CloseResponse {

}

message GetSpaceDataInfoRequest {

}
message GetRewardRangeRequest {

}
service EnvironmentService {
    // Run one timestep of the environment’s dynamics.
    rpc Step (StepRequest) returns (StepResponse);
    rpc MultiAgentStep (MultiAgentStepRequest) returns (MultiAgentStepResponse);

    // Resets the environment to an initial state and returns the initial observation.
    rpc Reset (ResetRequest) returns (ResetResponse);

    // Override close in your subclass to perform any necessary cleanup.
    rpc Close(CloseRequest) returns (CloseResponse);

    // Gets the number of training areas
    rpc GetNumAreas (GetNumAreasRequest) returns (GetNumAreasResponse);

    // Gives the format of valid actions
    rpc GetActionSpaceInfo (GetSpaceDataInfoRequest) returns (SpaceData);

    // Gives the format of valid observations
    rpc GetObservationSpaceInfo (GetSpaceDataInfoRequest) returns (SpaceData);

    // Gives the min and max possible rewards (if is_bounded is false, then assume -inf: inf)
    rpc GetRewardRange (GetRewardRangeRequest) returns (RangeInfo);
}
